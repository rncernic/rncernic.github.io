---
layout: post
title: "Teste"
author: "RN Cernic"
categories: journal
tags: [data science,algorithm,sample]
image: mountains.jpg
---

# Gradient descent

###  teste $$\frac{\partial x}{\partial w_{i}}$$

teste de fÃ³rmula inline $ i\hbar\frac{\partial}{\partial t} \Psi(\mathbf{r},t) = \left [ \frac{-\hbar^2}{2\mu}\nabla^2 + V(\mathbf{r},t)\right ] \Psi(\mathbf{r},t) $. Se funcionar tudo isso deve ficar na mesma linha.


```python
import numpy as np
```


```python
def lr_gradient_descent(x, y, step = 0.1, threshold = 0.0005):
    '''
    Function to preform a simple linear regression using gradient descent to optimize and with
    decreasing step size (aka learning rate). To decrease step size we divide it by sqrt(iteration).
    
    y = w1 * x + w0

    Parameters
    ----------
        x : array
            value of the features
        y :
        step :
            learning rate
        threshold :
        
    Returns
    -------
    w0 :
    w1 :
    rss : (cost function)
    mse : (cost function)
    
    '''
    w0 = w1  = 0
    n        = len(x)
    w1_d     = 1
    i        = 1
    
    while (np.abs(w1_d) > threshold):             # execute while RSS derivative w.r.t w1 is greater than threshold
        y_pred = w1 * x + w0                      # predicted y
        rss = sum(val**2 for val in (y - y_pred)) # residual sum of squares
        mse = (1/n)*rss                           # mean squared error
        w1_d = -(2/n)*sum(x * (y - y_pred))       # RSS derivative with respect to (w.r.t.) w1
        w0_d = -(2/n)*sum(y - y_pred)             # RSS derivative w.r.t. w0
        w1 = w1 - w1_d * step / np.sqrt(i)
        w0 = w0 - w0_d * step / np.sqrt(i)
        i += 1
        
    return w0, w1, rss, mse, i
```


```python
def lr_predict(x, w0, w1):
    #return list(w0 + val * w1 for val in x)
    return (w0 + w1*x) # vectorized form
```


```python
x = np.array([1, 2, 3, 4, 5])
y = np.array([5, 7, 9, 11, 13])

w0, w1, rss, mse, i = lr_gradient_descent(x, y, threshold=0.00005)
print ('wO: {}  w1: {}  rss: {}  mse: {}  iter: {}'.format(w0, w1, rss, mse, i))
```

    wO: 2.9994662521368896  w1: 2.0001478395994083  rss: 2.594130875399921e-07  mse: 5.188261750799843e-08  iter: 15431



```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.plot(x, y, '*',
         x, lr_predict(x, w0, w1), '-');
```


    
![png](output_5_0.png)
    


# Simple Linear Regression


```python
# define a class for the simple linear regression
class LinearRegression:
    '''
    
    A class to ...
    ...

    Attributes
    ----------
    learning_rate : float
        learning rate (a.k.a step) for the gradient descent
    num_iter : int
        Max number of iterations that can be executed

    Methods
    -------
    fit(X,y):
        Xx...

    '''
    
    def __init__ (self, learning_rate = 1e-3, num_iter = 1000):
        self.learnig_rate = learning_rate
        self.num_iter = num_iter
    
    def fit(self, X, y):
        '''
        Function ...

        Parameters
        ----------
            X : array
                value of the features
            y :

        Returns
        -------

        '''
        pass

    def predict(self):
        pass
    
    def get(self):
        pass
    
    def gradientDescent(self):
        pass
    
    def get(self):
        # get weights
        # get metrics
        pass
```


```python
model = LinearRegression(learning_rate=0.005)
```


```python
print(model.__doc__)
```

    
        
        A class to ...
        ...
    
        Attributes
        ----------
        learning_rate : float
            learning rate (a.k.a step) for the gradient descent
        num_iter : int
            Max number of iterations that can be executed
    
        Methods
        -------
        fit(X,y):
            Xx...
    
        



```python
help(model)
```

    Help on SimpleLinearRegression in module __main__ object:
    
    class SimpleLinearRegression(builtins.object)
     |  A class to ...
     |  ...
     |  
     |  Attributes
     |  ----------
     |  learning_rate : float
     |      learning rate (a.k.a step) for the gradient descent
     |  num_iter : int
     |      Max number of iterations that can be executed
     |  
     |  Methods
     |  -------
     |  fit(X,y):
     |      Xx...
     |  
     |  Methods defined here:
     |  
     |  __init__(self, learning_rate=0.001, num_iter=1000)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  fit(self, X, y)
     |      Function ...
     |      
     |      Parameters
     |      ----------
     |          X : array
     |              value of the features
     |          y :
     |      
     |      Returns
     |      -------
     |  
     |  get(self)
     |  
     |  gradientDescent(self)
     |  
     |  predict(self)
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
    



```python

```
